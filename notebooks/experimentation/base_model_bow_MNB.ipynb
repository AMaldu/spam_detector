{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to run BOW and Naive Bayes as our baseline model to have a reference in our experimentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.exceptions import RestException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='/home/maldu/dscience/projects/spam_detector/notebooks/experimentation/artifacts/1', creation_time=1733308701193, experiment_id='1', last_update_time=1733308701193, lifecycle_stage='active', name='spam-classifier', tags={'mlflow.note.content': 'This experiment contains mlruns for different '\n",
       "                         'approaches in the ml lifecycle of an e-mail spam '\n",
       "                         'detector classifier.',\n",
       "  'project_name': 'spam-classifier',\n",
       "  'project_quarter': 'Q4-2024',\n",
       "  'project_stage': 'testing',\n",
       "  'team': 'ml-team'}>,\n",
       " <Experiment: artifact_location='/home/maldu/dscience/projects/spam_detector/notebooks/experimentation/artifacts/0', creation_time=1733308493408, experiment_id='0', last_update_time=1733308493408, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on the console\n",
    "# mlflow server --backend-store-uri sqlite:///backend.db --default-artifact-root ./artifacts\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.search_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlflow.tracking import MlflowClient\n",
    "# client = MlflowClient()\n",
    "# client.delete_run(\"81b0d40ddc814076a95bc6fd9d4fff34\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 'spam-classifier' already exists.\n",
      "Working with experiment ID: 1\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"spam-classifier\"\n",
    "\n",
    "experiment_description = (\n",
    "    \"This experiment contains mlruns for different approaches in the ml lifecycle of an e-mail spam detector classifier.\"\n",
    ")\n",
    "\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"spam-classifier\",\n",
    "    \"project_stage\": \"testing\",\n",
    "    \"team\": \"ml-team\",\n",
    "    \"project_quarter\": \"Q4-2024\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(name=experiment_name, tags=experiment_tags)\n",
    "    print(f\"Experiment created with ID: {experiment_id}\")\n",
    "except RestException as e:\n",
    "    if \"RESOURCE_ALREADY_EXISTS\" in str(e):\n",
    "        print(f\"Experiment '{experiment_name}' already exists.\")\n",
    "        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(f\"Working with experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maldu/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/mlflow/types/utils.py:407: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"../../data/gold/train.csv\")\n",
    "test = pd.read_csv(\"../../data/gold/test.csv\")\n",
    "\n",
    "X_train = train['features']\n",
    "y_train = train['target']\n",
    "X_test = test['features']\n",
    "y_test = test['target']\n",
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    fbeta_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "artifact_root = \"./artifacts_local/mlflow_artifacts\"\n",
    "os.makedirs(artifact_root, exist_ok=True)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(ngram_range=(1, 1), max_features=2000)),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "pipeline.fit(X_train, y_train)\n",
    "y_test_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Test Data): 0.97 \n",
      "Balanced Accuracy (Test Data): 0.96\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Accuracy (Test Data): {test_accuracy:.2f} \")\n",
    "print(f\"Balanced Accuracy (Test Data): {balanced_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F0.5-Score: 0.8451\n",
      "Classification Report (Test Data):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       453\n",
      "           1       0.82      0.95      0.88        63\n",
      "\n",
      "    accuracy                           0.97       516\n",
      "   macro avg       0.91      0.96      0.93       516\n",
      "weighted avg       0.97      0.97      0.97       516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F0.5-Score\n",
    "f0_5_score = fbeta_score(y_test, y_test_pred, beta=0.5)\n",
    "print(f\"F0.5-Score: {f0_5_score:.4f}\")\n",
    "\n",
    "test_classification_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(\"Classification Report (Test Data):\")\n",
    "print(test_classification_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "- Precision on 1: predicts class 1 82% of the times\n",
    "- Recall on 1: the actual class 1 instances were correctly classified as 1\n",
    "- F1-score: the model is doing a decent job on the positive class as well.\n",
    "\n",
    "- Accuracy and macro avg can be ignored\n",
    "\n",
    "- weighted avg \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=test_conf_matrix, display_labels=['Ham', 'Spam'])\n",
    "cm_display.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Test Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_test_pred_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(f\"Precision-Recall AUC (Test Data): {pr_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='b', label=f'PR AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve (Test Data)')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MlFlow tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline logged to MLflow under run ID cb986f7efd3d42eb8a5d327af93ba59f\n",
      "üèÉ View run baseline-model at: http://127.0.0.1:5000/#/experiments/1/runs/cb986f7efd3d42eb8a5d327af93ba59f\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "with mlflow.start_run(run_name=\"baseline-model\") as run:\n",
    "    \n",
    "    mlflow.set_tag(\"model\", \"Reference model MultinomialNB + BOW\")\n",
    "\n",
    "    #Datasets\n",
    "    mlflow.log_param(\"data_folder\", \"../../data/gold/\")\n",
    "    mlflow.log_param(\"train_file\", \"train.csv\")\n",
    "    mlflow.log_param(\"test_file\", \"test.csv\")\n",
    "    \n",
    "    #BOW and model\n",
    "    mlflow.log_param(\"vectorizer_type\", \"CountVectorizer\")\n",
    "    mlflow.log_param(\"model_type\", \"MultinomialNB\")\n",
    "    \n",
    "    # Metrics\n",
    "    mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"train_roc_auc\", train_roc_auc)\n",
    "    mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n",
    "    mlflow.log_metric(\"train_f1\", train_f1)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    \n",
    "    #Artifacts\n",
    "    mlflow.log_artifact(train_conf_matrix_path, artifact_path=\"confusion_matrices\")\n",
    "    mlflow.log_artifact(test_conf_matrix_path, artifact_path=\"confusion_matrices\")\n",
    "    mlflow.log_artifact(roc_curve_path, artifact_path=\"roc_curves\")\n",
    "\n",
    "    \n",
    "    # Pipeline\n",
    "    mlflow.sklearn.log_model(pipeline, \"pipeline\",signature=signature)\n",
    "    \n",
    "\n",
    "    print(f\"Pipeline logged to MLflow under run ID {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- Clear overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam-detector-P2ybB3t6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
