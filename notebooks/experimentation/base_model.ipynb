{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to run BOW and Naive Bayes as our baseline model to have a reference in our experimentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.exceptions import RestException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='/home/maldu/dscience/projects/spam_detector/notebooks/experimentation/artifacts_local/1', creation_time=1733152016926, experiment_id='1', last_update_time=1733152016926, lifecycle_stage='active', name='spam-classifier', tags={'mlflow.note.content': 'This experiment contains mlruns for different '\n",
       "                         'approaches in the ml lifecycle of an e-mail spam '\n",
       "                         'detector classifier.',\n",
       "  'project_name': 'spam-classifier',\n",
       "  'project_quarter': 'Q4-2024',\n",
       "  'project_stage': 'testing',\n",
       "  'team': 'ml-team'}>,\n",
       " <Experiment: artifact_location='/home/maldu/dscience/projects/spam_detector/notebooks/experimentation/artifacts_local/0', creation_time=1733152009293, experiment_id='0', last_update_time=1733152009293, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on the console\n",
    "# mlflow server --backend-store-uri=mlruns sqlite:///backend.db --default-artifact-root ./artifacts_local\n",
    "#--------------\n",
    "# optimize\n",
    "# $ mlflow server \\\n",
    "    # --backend-store-uri=mlruns \\\n",
    "    # --artifacts-destination ./mlartifacts \\\n",
    "    # --default-artifact-root http://localhost:5000/api/2.0/mlflow-artifacts/artifacts/experiments \\\n",
    "    # --gunicorn-opts \"--log-level debug\"\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.search_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 'spam-classifier' already exists.\n",
      "Working with experiment ID: 1\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"spam-classifier\"\n",
    "\n",
    "experiment_description = (\n",
    "    \"This experiment contains mlruns for different approaches in the ml lifecycle of an e-mail spam detector classifier.\"\n",
    ")\n",
    "\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"spam-classifier\",\n",
    "    \"project_stage\": \"testing\",\n",
    "    \"team\": \"ml-team\",\n",
    "    \"project_quarter\": \"Q4-2024\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(name=experiment_name, tags=experiment_tags)\n",
    "    print(f\"Experiment created with ID: {experiment_id}\")\n",
    "except RestException as e:\n",
    "    if \"RESOURCE_ALREADY_EXISTS\" in str(e):\n",
    "        print(f\"Experiment '{experiment_name}' already exists.\")\n",
    "        experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "print(f\"Working with experiment ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maldu/.cache/pypoetry/virtualenvs/spam-detector-P2ybB3t6-py3.10/lib/python3.10/site-packages/mlflow/types/utils.py:407: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"../../data/gold/train.csv\")\n",
    "test = pd.read_csv(\"../../data/gold/test.csv\")\n",
    "\n",
    "X_train = train['features']\n",
    "y_train = train['target']\n",
    "X_test = test['features']\n",
    "y_test = test['target']\n",
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "artifact_root = \"./artifacts_local/mlflow_artifacts\"\n",
    "os.makedirs(artifact_root, exist_ok=True)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(ngram_range=(1, 1), max_features=2000)),\n",
    "        ('classifier', MultinomialNB())\n",
    "    ])\n",
    "    \n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "y_train_prob = pipeline.predict_proba(X_train)[:, 1]\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "train_roc_auc = roc_auc_score(y_train, y_train_prob)\n",
    "train_f1 = f1_score(y_train, y_train_pred)\n",
    "\n",
    "\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "y_test_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_prob)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_prob)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Training):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3606\n",
      "           1       0.95      0.95      0.95       517\n",
      "\n",
      "    accuracy                           0.99      4123\n",
      "   macro avg       0.97      0.97      0.97      4123\n",
      "weighted avg       0.99      0.99      0.99      4123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nClassification Report (Training):\")\n",
    "print(classification_report(y_train, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       453\n",
      "           1       0.82      0.95      0.88        63\n",
      "\n",
      "    accuracy                           0.97       516\n",
      "   macro avg       0.91      0.96      0.93       516\n",
      "weighted avg       0.97      0.97      0.97       516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_train = confusion_matrix(y_train, y_train_pred)\n",
    "disp_train = ConfusionMatrixDisplay(conf_matrix_train, display_labels=pipeline.classes_)\n",
    "disp_train.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Train\")\n",
    "train_conf_matrix_path = os.path.join(artifact_root, \"conf_matrix_train.png\")\n",
    "plt.savefig(train_conf_matrix_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_test = confusion_matrix(y_test, y_test_pred)\n",
    "disp_test = ConfusionMatrixDisplay(conf_matrix_test, display_labels=pipeline.classes_)\n",
    "disp_test.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Test\")\n",
    "test_conf_matrix_path = os.path.join(artifact_root, \"conf_matrix_test.png\")\n",
    "plt.savefig(test_conf_matrix_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9866601988843076\n",
      "Test Accuracy: 0.9689922480620154\n",
      "\n",
      "Training AUC: 0.9915952458346341\n",
      "Test AUC: 0.9844773818283752\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(f\"\\nTraining AUC: {train_roc_auc}\")\n",
    "print(f\"Test AUC: {test_roc_auc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr_train, tpr_train, label=f'Training AUC = {train_roc_auc:.2f}')\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test AUC = {test_roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  \n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Train and Test ROC Curves')\n",
    "plt.legend(loc='lower right')\n",
    "roc_curve_path = os.path.join(artifact_root, \"roc_curve.png\")\n",
    "plt.savefig(roc_curve_path)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MlFlow tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/02 18:57:21 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "2024/12/02 18:57:22 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/12/02 18:57:22 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline logged to MLflow under run ID 81b0d40ddc814076a95bc6fd9d4fff34\n",
      "🏃 View run baseline-model at: http://127.0.0.1:5000/#/experiments/1/runs/81b0d40ddc814076a95bc6fd9d4fff34\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "with mlflow.start_run(run_name=\"baseline-model\", log_system_metrics=True) as run:\n",
    "    \n",
    "    mlflow.set_tag(\"Reference model MultinomialNB + BOW\")\n",
    "\n",
    "    #Datasets\n",
    "    mlflow.log_param(\"data_folder\", \"../data/gold/\")\n",
    "    mlflow.log_param(\"train_file\", \"train.csv\")\n",
    "    mlflow.log_param(\"test_file\", \"test.csv\")\n",
    "    \n",
    "    #BOW and model\n",
    "    mlflow.log_param(\"vectorizer_type\", \"CountVectorizer\")\n",
    "    mlflow.log_param(\"model_type\", \"MultinomialNB\")\n",
    "    \n",
    "    # Metrics\n",
    "    mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"train_roc_auc\", train_roc_auc)\n",
    "    mlflow.log_metric(\"test_roc_auc\", test_roc_auc)\n",
    "    mlflow.log_metric(\"train_f1\", train_f1)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    \n",
    "    #Artifacts\n",
    "    mlflow.log_artifact(train_conf_matrix_path, artifact_path=\"confusion_matrices\")\n",
    "    mlflow.log_artifact(test_conf_matrix_path, artifact_path=\"confusion_matrices\")\n",
    "    mlflow.log_artifact(roc_curve_path, artifact_path=\"roc_curves\")\n",
    "\n",
    "    \n",
    "    # Pipeline\n",
    "    mlflow.sklearn.log_model(pipeline, \"pipeline\",signature=signature)\n",
    "    \n",
    "\n",
    "    print(f\"Pipeline logged to MLflow under run ID {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "- Clear overfitting since precision ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam-detector-P2ybB3t6-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
